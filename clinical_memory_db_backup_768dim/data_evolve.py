# -*- coding: utf-8 -*-
"""
EHR â†’ ç—…æƒ…ç®¡ç†è®°å¿†åº“ï¼ˆJSONç‰ˆï¼‰
è¯»å– example.jsonï¼Œæå–ä¸€çº§/äºŒçº§æ ¸å¿ƒä¿¡æ¯ï¼Œ
å¹¶è°ƒç”¨ Qwen2.5-Med ç”Ÿæˆ**æ€»ä½“**ç—…æƒ…æ‘˜è¦ï¼Œè¾“å‡º
{subject_id}_clinical_memory.json
"""
import json
import datetime
from pydoc import cli
import sys
from typing import Dict, List, Any
from pathlib import Path
from openai import OpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
import re

# -------------------------- é…ç½® --------------------------
INPUT_JSON = "breast.json"          # åŒç›®å½•ä¸‹çš„ MIMIC ç¤ºä¾‹
OUTPUT_DIR = Path("memory_output")   # è¾“å‡ºæ–‡ä»¶å¤¹
OUTPUT_DIR.mkdir(exist_ok=True)

LLM_API_KEY = os.getenv("QWEN_API_KEY")
LLM_BASE_URL = os.getenv("BASE_URL")
LLM_MODEL = "qwen-plus"

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

# -------------------------- å‘é‡åº“é…ç½® --------------------------
FAISS_DEVICE = "cuda"  # ä½¿ç”¨ GPU è®¾å¤‡ä»¥åŠ é€ŸåµŒå…¥è®¡ç®—
FAISS_DB_PATH = "clinical_memory_db"  # å…¨å±€åº“è·¯å¾„ï¼ˆæ‰€æœ‰æ‚£è€…å…±ç”¨ï¼‰

def get_embeddings():
    """åˆå§‹åŒ–ä¸­æ–‡å¥å‘é‡æ¨¡å‹ï¼ˆä¸ demo.py ä¿æŒä¸€è‡´çš„ç³»åˆ—ï¼‰"""
    return SentenceTransformerEmbeddings(
        model_name="BAAI/bge-base-zh-v1.5",
        model_kwargs={"device": FAISS_DEVICE},
        encode_kwargs={"normalize_embeddings": True}
    )

# ---------- åŸè„šæœ¬å·¥å…·å‡½æ•°ï¼ˆå®Œæ•´æ¬è¿‡æ¥ï¼‰ ----------
def parse_json_data(json_str, data_type):
    try:
        # å…¼å®¹å­—ç¬¦ä¸²/åˆ—è¡¨/None
        if json_str is None:
            return []
        if isinstance(json_str, list):
            data_list = json_str
        else:
            s = str(json_str).strip()
            if s in ["[]", "null", ""]:
                return []
            cleaned = s.replace('""', '"')
            if not cleaned.startswith('['):
                cleaned = f"[{cleaned}]"
            data_list = json.loads(cleaned)
        parsed = []
        for item in data_list:
            time_str = item.get("charttime") or item.get("starttime") or item.get("storetime") or item.get("chart_time")
            if not time_str:
                continue
            try:
                charttime = datetime.datetime.fromisoformat(time_str)
                date = charttime.strftime("%Y-%m-%d")
            except ValueError:
                continue
            # åç§°å­—æ®µå…¼å®¹
            if data_type == "lab":
                item_name = item.get("lab_name") or str(item.get("itemid", "Unknown_Lab"))
            else:
                item_name = item.get("vital_name") or item.get("label") or str(item.get("itemid", "Unknown_Vital"))
            # æ•°å€¼è§£æï¼šä¼˜å…ˆ valuenumï¼›å¦åˆ™ä» value ä¸­æå–æ•°å­—
            val = item.get("valuenum")
            if isinstance(val, str):
                try:
                    val = float(val)
                except Exception:
                    val = None
            if val is None:
                v_str = item.get("value")
                if isinstance(v_str, (str, bytes)):
                    if isinstance(v_str, bytes):
                        v_str = v_str.decode("utf-8", errors="ignore")
                    m = re.search(r"([+-]?\d+(?:\.\d+)?)", v_str)
                    if m:
                        try:
                            val = float(m.group(1))
                        except Exception:
                            val = None
            item_info = {
                "date": date,
                "data_type": data_type,
                "item_name": item_name,
                "valuenum": val,
                "valueuom": item.get("valueuom", "") or item.get("uom", ""),
                "flag": item.get("flag")
            }
            if isinstance(item_info["valuenum"], (int, float)):
                parsed.append(item_info)
        return parsed
    except Exception as e:
        print(f"[{data_type}] JSONè§£æå¤±è´¥: {e}")
        return []

def aggregate_daily_data(parsed_labs, parsed_vitals):
    daily_data = {}
    for item in parsed_labs + parsed_vitals:
        date = item["date"]
        key = f"{item['data_type']}_{item['item_name'].replace(' ', '_').replace('/', '_')}"
        daily_data.setdefault(date, {}).setdefault(key, {"values": [], "uom": item["valueuom"], "flag": None})
        if isinstance(item.get("valuenum"), (int, float)):
            daily_data[date][key]["values"].append(float(item["valuenum"]))
        if item["flag"] == "abnormal":
            daily_data[date][key]["flag"] = "abnormal"
    aggregated = []
    for date in sorted(daily_data):
        day_dict = {"date": date}
        for k, v in daily_data[date].items():
            if not v["values"]:
                continue
            mean_val = round(sum(v["values"]) / max(len(v["values"]), 1), 2)
            day_dict[f"{k}_mean"] = mean_val
            day_dict[f"{k}_uom"] = v["uom"]
            day_dict[f"{k}_flag"] = v["flag"] or "normal"
        aggregated.append(day_dict)
    return aggregated

# ---------- å®‰å…¨ JSON å·¥å…· ----------
def safe_list_from_json(value):
    """å°†å¯èƒ½ä¸º None/ç©ºä¸²/JSONä¸²/å·²æ˜¯åˆ—è¡¨ çš„è¾“å…¥å®‰å…¨è½¬ä¸º listã€‚
    è§£æå¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    try:
        if value is None:
            return []
        if isinstance(value, list):
            return value
        if isinstance(value, (bytes, bytearray)):
            value = value.decode("utf-8", errors="ignore")
        if isinstance(value, str):
            v = value.strip()
            if v == "" or v.lower() in ("null", "none"):
                return []
            return json.loads(v)
        # å…¶ä»–ç±»å‹ç›´æ¥ç©ºåˆ—è¡¨
        return []
    except Exception:
        return []

# ---------- LLM ----------
def call_qwen(prompt: str) -> str:
    try:
        rsp = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸´åºŠæ•°æ®åˆ†æå¸ˆï¼Œéœ€åŸºäºæ‚£è€…EHRçš„æ¯æ—¥å®éªŒå®¤æŒ‡æ ‡å’Œç”Ÿå‘½ä½“å¾ï¼Œå®¢è§‚åˆ†æç—…æƒ…å˜åŒ–ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1200,
            temperature=0.3
        )
        return rsp.choices[0].message.content.strip()
    except Exception as e:
        return f"ã€LLMå¼‚å¸¸ã€‘{e}"

# ---------- æ ¸å¿ƒ ----------
def load_patients() -> List[Dict[str, Any]]:
    with open(INPUT_JSON, encoding="utf-8") as f:
        return json.load(f)

def overall_summary(p_info: Dict, daily_agg: List[Dict]) -> Dict[str, str]:
    """ç”Ÿæˆä½é™¢æœŸé—´æ€»ä½“æ€»ç»“ï¼ˆæ— ç»“æ„åŒ–æ—¥è®°å½•æ—¶ä½¿ç”¨å‡ºé™¢ç”Ÿå‘½ä½“å¾ä½œä¸ºä¾æ®ï¼‰"""
    if not daily_agg:
        # æ— ç»“æ„åŒ–æ—¥è®°å½•ï¼šä¾æ® baseline_vitals ç»™å‡ºç®€è¦è¯„ä¼°
        bv = p_info.get("baseline_vitals", {}) if isinstance(p_info, dict) else {}
        parts = []
        if "temp" in bv:
            parts.append(f"ä½“æ¸©{bv['temp']}")
        if "heart_rate" in bv:
            parts.append(f"å¿ƒç‡{bv['heart_rate']}")
        if "sbp" in bv and "dbp" in bv:
            parts.append(f"è¡€å‹{bv['sbp']}/{bv['dbp']}")
        if "resp_rate" in bv:
            parts.append(f"å‘¼å¸{bv['resp_rate']}")
        if "spo2" in bv:
            parts.append(f"è¡€æ°§{bv['spo2']}%")
        if parts:
            return {"summary": "ç»“æ„åŒ–è®°å½•è¾ƒå°‘ï¼Œä¾æ®å‡ºé™¢ç”Ÿå‘½ä½“å¾ï¼š" + "ï¼Œ".join(parts) + "ã€‚æ•´ä½“è¶‹äºç¨³å®šã€‚", "trend": "stable"}
        return {"summary": "ç»“æ„åŒ–è®°å½•è¾ƒå°‘ï¼Œæ•´ä½“è¶‹äºç¨³å®šã€‚", "trend": "stable"}

    chunks = []
    for day in daily_agg:
        d = day["date"]
        labs = [f"{k}:{v}" for k, v in day.items() if k.startswith("lab_") and "_mean" in k]
        vitals = [f"{k}:{v}" for k, v in day.items() if k.startswith("vital_") and "_mean" in k]
        chunks.append(f"{d} å®éªŒå®¤ï¼š{'ï¼›'.join(labs)} ç”Ÿå‘½ä½“å¾ï¼š{'ï¼›'.join(vitals)}")
    prompt = f"""
æ‚£è€…{p_info['gender']}{p_info['anchor_age']}å²ï¼Œè¯Šæ–­{p_info['chronic_diseases'][:200]}ã€‚
ä½é™¢æœŸé—´æŒ‡æ ‡å¦‚ä¸‹ï¼š
""" + "\n".join(chunks) + """
è¯·ç”¨2-3å¥è¯æ€»ä½“æ€»ç»“ä½é™¢æœŸé—´ç—…æƒ…å˜åŒ–è¶‹åŠ¿ï¼ˆæ”¹å–„/æ¶åŒ–/ç¨³å®šï¼‰ï¼Œå¹¶æŒ‡å‡ºæœ€å…³é”®å¼‚å¸¸ã€‚åªç»™æ€»ç»“ï¼Œä¸å±•å¼€å»ºè®®ã€‚"""
    summary = call_qwen(prompt)
    trend = "improve" if "æ”¹å–„" in summary else "worsen" if "æ¶åŒ–" in summary else "stable"
    return {"summary": summary, "trend": trend}

def build_memory(patient: Dict) -> Dict:
    mem = {
        "subject_id": patient["subject_id"],
        "gender": patient["gender"],
        "anchor_age": patient["anchor_age"],
        "allergies": [],
        "cancer_history": [],
        "chronic_diseases": [],
        "baseline_labs": {},
        "baseline_vitals": {},
        "discharge_medications": [],
        "daily_summaries": {}
    }

    # è¿‡æ•
    try:
        discharge_list = safe_list_from_json(patient.get("discharge_json"))
        if discharge_list:
            note = discharge_list[0].get("text_snippet", "")
            if "Allergies:" in note:
                mem["allergies"] = [a.strip() for a in note.split("Allergies:")[1].split("\n")[0].split("/")]
    except Exception:
        pass

    # è¯Šæ–­
    try:
        for d in safe_list_from_json(patient.get("diagnoses")):
            code = str(d.get("icd_code", "")).strip()
            desc = str(d.get("desc", "")).strip()
            if not code:
                continue
            if code.startswith("1"):
                mem["cancer_history"].append({"icd9": code, "desc": desc})
            elif code[:1] in "2345":
                mem["chronic_diseases"].append({"icd9": code, "desc": desc})
    except Exception:
        pass

    # åŸºçº¿æ£€éªŒ & ç”Ÿå‘½ä½“å¾
    labs = safe_list_from_json(patient.get("labs_json"))
    baseline_labs = {}
    for k in labs:
        name = k.get("lab_name") or str(k.get("itemid", "")).strip()
        if not name:
            continue
        if k.get("flag") == "abnormal":
            continue
        val = k.get("valuenum")
        if isinstance(val, str):
            try:
                val = float(val)
            except Exception:
                val = None
        if val is None:
            v_str = k.get("value")
            if isinstance(v_str, (str, bytes)):
                if isinstance(v_str, bytes):
                    v_str = v_str.decode("utf-8", errors="ignore")
                m = re.search(r"([+-]?\d+(?:\.\d+)?)", v_str)
                if m:
                    try:
                        val = float(m.group(1))
                    except Exception:
                        val = None
        if not isinstance(val, (int, float)):
            continue
        baseline_labs[name.lower().replace(" ", "_")] = val
    mem["baseline_labs"] = baseline_labs
    # è§£æå‡ºé™¢è®°å½• VS ç”Ÿå‘½ä½“å¾
    discharge_list = safe_list_from_json(patient.get("discharge_json"))
    if discharge_list:
        note = discharge_list[0].get("text_snippet", "")
        m_line = re.search(r"VS:.*", note)
        vit = {}
        if m_line:
            line = m_line.group(0)
            tokens = re.split(r"\s+", line.replace("VS:", "").strip())
            try:
                if len(tokens) >= 1:
                    t = re.search(r"([+-]?\d+(?:\.\d+)?)", tokens[0])
                    if t:
                        vit["temp"] = float(t.group(1))
                if len(tokens) >= 2:
                    h = re.search(r"(\d+)", tokens[1])
                    if h:
                        vit["heart_rate"] = int(h.group(1))
                if len(tokens) >= 3 and "/" in tokens[2]:
                    sbp_dbp = tokens[2].split("/")
                    try:
                        vit["sbp"] = int(re.sub(r"\D", "", sbp_dbp[0]))
                        vit["dbp"] = int(re.sub(r"\D", "", sbp_dbp[1]))
                    except Exception:
                        pass
                if len(tokens) >= 4:
                    r = re.search(r"(\d+)", tokens[3])
                    if r:
                        vit["resp_rate"] = int(r.group(1))
                if len(tokens) >= 5:
                    s = re.search(r"(\d+)", tokens[4])
                    if s:
                        vit["spo2"] = int(s.group(1))
            except Exception:
                pass
        if vit:
            mem["baseline_vitals"] = vit
    if not mem["baseline_vitals"]:
        mem["baseline_vitals"] = {k: v for k, v in {"heart_rate": 75, "sbp": 120, "dbp": 80, "spo2": 98}.items()}

    # å‡ºé™¢å¸¦è¯
    try:
        rx = safe_list_from_json(patient.get("prescriptions_json"))
        mem["discharge_medications"] = [{"drug": r.get("drug"), "dose": r.get("dose_val_rx"), "route": r.get("route")} for r in rx]
    except Exception:
        pass

    # æ€»ä½“æ‘˜è¦
    parsed_labs = parse_json_data(patient["labs_json"], "lab")
    parsed_vitals = parse_json_data(patient["vitals_json"], "vital")
    daily_agg = aggregate_daily_data(parsed_labs, parsed_vitals)
    mem["daily_summaries"] = overall_summary(mem, daily_agg)
    return mem


def save_patient_memory_to_faiss(subject_id: str, memory_obj: Dict[str, Any]) -> str:
    """å°†æ‚£è€…ç—…æƒ… JSON ä¿å­˜åˆ°å…¨å±€ FAISS åº“ï¼ˆç»Ÿä¸€ç´¢å¼•ï¼ŒæŒ‰ subject_id æ ‡æ³¨ï¼‰"""
    embeddings = get_embeddings()
    text = json.dumps(memory_obj, ensure_ascii=False)
    doc_id = f"clinical_memory_{subject_id}"

    # è‹¥åº“å·²å­˜åœ¨åˆ™åŠ è½½å¹¶è¿½åŠ ï¼›å¦åˆ™ä»¥å½“å‰æ–‡æ¡£åˆå§‹åŒ–
    if Path(FAISS_DB_PATH).exists():
        db = FAISS.load_local(FAISS_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        db.add_texts([text], metadatas=[{"subject_id": subject_id}], ids=[doc_id])
    else:
        db = FAISS.from_texts([text], embeddings, metadatas=[{"subject_id": subject_id}], ids=[doc_id])
    db.save_local(FAISS_DB_PATH)
    return FAISS_DB_PATH


def get_memory_by_subject_id(subject_id: str) -> Dict[str, Any]:
    """åœ¨å…¨å±€ FAISS åº“ä¸­æŒ‰ subject_id è·å–å¯¹åº”æ‚£è€…çš„ JSON å†…å®¹"""
    embeddings = get_embeddings()
    if not Path(FAISS_DB_PATH).exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°å…¨å±€FAISSæ•°æ®åº“ï¼š{FAISS_DB_PATH}")
    db = FAISS.load_local(FAISS_DB_PATH, embeddings, allow_dangerous_deserialization=True)

    # 1) å…ˆæŒ‰å”¯ä¸€IDå‘½ä¸­
    doc_id = f"clinical_memory_{subject_id}"
    doc = None
    try:
        doc = db.docstore.search(doc_id)
    except Exception:
        doc = getattr(db.docstore, "_dict", {}).get(doc_id)

    # 2) å›é€€ï¼šéå†æŒ‰ metadata åŒ¹é… subject_id
    if not doc and hasattr(db.docstore, "_dict"):
        for d in db.docstore._dict.values():
            if isinstance(getattr(d, "metadata", None), dict) and d.metadata.get("subject_id") == subject_id:
                doc = d
                break

    if not doc:
        raise RuntimeError(f"å…¨å±€åº“ä¸­æœªæ‰¾åˆ° subject_id={subject_id} çš„ä¸´åºŠè®°å¿†æ–‡æ¡£")

    try:
        return json.loads(doc.page_content)
    except Exception:
        return {"raw": doc.page_content, "metadata": doc.metadata}

def main():
    pts = load_patients()
    for p in pts:
        sid = p["subject_id"]
        memory = build_memory(p)
        out = OUTPUT_DIR / "clinical_memory" / f"{sid}_clinical_memory.json"
        with out.open("w", encoding="utf-8") as f:
            json.dump(memory, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ç”Ÿæˆ {out}")

        # ä¿å­˜åˆ° FAISSï¼ˆæ‰€æœ‰æ‚£è€…å…±ç”¨åŒä¸€åº“ï¼‰
        db_path = save_patient_memory_to_faiss(sid, memory)
        print(f"ğŸ“¦ å·²ä¿å­˜åˆ°å…¨å±€ FAISS æ•°æ®åº“: {db_path}")

if __name__ == "__main__":
    patient_memory = get_memory_by_subject_id(11489167)
    print(patient_memory)
    # main()